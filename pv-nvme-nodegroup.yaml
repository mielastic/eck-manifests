apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  # Replace with your EKS Cluster's name
  name: workday-test-muhammad
  # Replace with the AWS Region your cluster is deployed in
  region: us-east-2

vpc:
  id: "vpc-9c8480f4"
  securityGroup: "sg-07bbf578b33f95014"    # this is the ControlPlaneSecurityGroup
  subnets:
    private:
      private1:
          id: "subnet-000b760964b4628d5"
      private2:
          id: "subnet-05ad069322b7e18be"
      private3:
          id: "subnet-0482b4696f6b58992"
    public:
      public1:
          id: "subnet-000b760964b4628d5"
      public2:
          id: "subnet-05ad069322b7e18be"
      public3:
          id: "subnet-0482b4696f6b58992"

managedNodeGroups:
  # Name to give the managed node-group
  - name: eks-pv-nvme-ng
    # Label the nodes that they contain fast-disks 
    labels: { fast-disk-node: "pv-nvme" }
    instanceType: i3.xlarge
    desiredCapacity: 2
    volumeSize: 100 # EBS Boot Volume size
    privateNetworking: true
    preBootstrapCommands:
      - |
        # Install NVMe CLI
        yum install nvme-cli -y
        
        # Get a list of instance-store NVMe drives
        nvme_drives=$(nvme list | grep "Amazon EC2 NVMe Instance Storage" | cut -d " " -f 1 || true)
        readarray -t nvme_drives <<< "$nvme_drives"

        for disk in "${nvme_drives[@]}"
        do
          # Format the disk to ext4
          mkfs.ext4 -F $disk
          
          # Get disk UUID
          uuid=$(blkid -o value -s UUID $disk)
   
          # Create a filesystem path to mount the disk
          mount_location="/mnt/fast-disks/${uuid}"
          mkdir -p $mount_location
   
          # Mount the disk
          mount $disk $mount_location
          
          # Mount the disk during a reboot
          echo $disk $mount_location ext4 defaults,noatime 0 2 >> /etc/fstab 
        done
